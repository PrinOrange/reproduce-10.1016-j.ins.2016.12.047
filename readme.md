# 结合标签相关性和用户社交关系进行微博推荐

## 论文概要

本论文主要研究以下几个问题：

- 很多用户懒得打标签
- 用户兴趣会变化
- 单纯看关注列表不够精准，自己关注的用户感兴趣的内容自己不一定感兴趣

所以解决方案是结合标签 + 社交关系进行建模设计推荐算法

## 标签提取与分析

这段内容是关于**用户标签检索（User tag
retrieval）**，目标是判断某个词（term）是否值得作为用户标签，主要依据：

1. **重要性（Importance）**：通过常见的 TF（词频）或 TF-IDF（词频 -
   逆文档频率）衡量；
2. **主题指示性（Topical Indication）**：通过 _clarity score_
   来衡量某个词是否与特定主题紧密相关。

公式是关于 **Clarity Score** 的计算：

$$ \text{Clarity}(l_{ij}) = \sum_{l_{ij}' \in \theta} P(l_{ij}'|Q_{ij}) \log
\frac{P(l_{ij}'|Q_{ij})}{P(l_{ij}'|D_i)} $$

其中：

- $l_{ij}$：第 $j$ 个候选词，来自用户 $i$。
- $Q_{ij}$：以词 $l_{ij}$ 为查询词，从整个微博集合中检索出的前 $g_i$
  个相关微博，组成的查询模型。
- $D_i$：用户 $i$ 的所有微博集合，作为集合模型（collection model）。
- $\theta$：词汇空间（所有可能出现的词）。
- $P(l_{ij}'|Q_{ij})$：在查询模型中，词 $l_{ij}'$ 的概率。
- $P(l_{ij}'|D_i)$：在整个用户微博集合中的概率。

通俗理解，这个公式其实就是在衡量：某个词作为查询词时，检索结果中的“词分布”与整个用户微博集合中的“词分布”**有多大差别**。

这个差别越大，表示这个词更**有指向性**、更有**主题聚焦能力**，即——更能代表一个特定的兴趣或话题。所以：

- 如果 $P(l_{ij}'|Q_{ij})$ 与 $P(l_{ij}'|D_i)$ 差异很大，clarity
  得分就高，说明该词作为标签很有代表性。
- 如果两个分布差不多（即检索结果跟整个集合没什么区别），说明这个词没有什么聚焦能力，clarity
  得分低。

可以这样理解 clarity score：

- 它是一个 **KL 散度（Kullback-Leibler Divergence）** 的形式。
- 衡量的是：一个查询引发的文档分布（局部）与整个集合分布（整体）之间的差异。
- 差异越大，说明这个查询词更“清晰”地定位了某个话题。

这个公式描述了如何利用 **Clarity Score** 和 **词频 tf**
来给每个候选词打分，并最终选出作为用户标签的词。

$$ s_j = tf_j \times \text{clarity}(l_{ij}) $$

- $s_j$：第 $j$ 个候选词的最终得分（score）。
- $tf_j$：该词在用户微博集合中的词频（Term Frequency）。
- $\text{clarity}(l_{ij})$：前面我们计算出的 clarity
  score，衡量该词的主题指示性。

这个公式结合了**词频（常用程度）** 和 **主题清晰度（聚焦性）**，综合得出一个评分
$s_j$，代表这个词的标签价值。

$$ \text{normalized}(s_j) = \frac{s_j}{\sum_{x=1}^{n_i} s_x} $$

- $n_i$：为用户 $u_i$ 最终选出的前 $n_i$ 个得分最高的词作为标签。
- $\text{normalized}(s_j)$：是对 $s_j$ 归一化后的结果，使所有标签的得分加起来为
  1，方便后续做权重计算或可视化。

这是对前面计算出的得分进行**归一化处理**，保证所有最终入选的标签得分之和为
1，可以作为权重分配给标签。

可以把这一整套流程理解成：

1. 看词有多常出现（tf）
2. 看这个词有多聚焦于一个主题（clarity）
3. 综合得分后选出得分最高的一批词
4. 给这些词进行归一化，作为用户标签及其权重

## 标签矩阵构建

这一步是将用户对标签的偏好量化为矩阵，用于后续分析。

首先是
**定义标签权重**。标签有两种方法：一是用户主动标记，二是根据词频分析自动计算（需要使用分词算法，中文分词算法有结巴分词等）。

- **用户主动标注**：若标签由用户直接提供，每个标签权重为
  $\frac{1}{Z_i}$（均分权重）。 $$w_{ij} = \frac{1}{Z_i} \quad
  (\text{用户直接标注时})$$
- **通过检索生成**：若标签通过算法生成（如 TF-IDF），需归一化处理。 $$w_{ij} =
  \text{normalized}(s_j) \quad (\text{检索生成时})$$

接下来是 **构建用户 -
标签矩阵**，即将每个用户的标签权重向量堆叠为矩阵，行表示用户，列表示标签：
$$M_{nl} = \begin{bmatrix} \vec{V}_1 \\ \vec{V}_2 \\ \vdots \\ \vec{V}_N
\end{bmatrix}$$

矩阵大小为 $N \times n$，$N$ 为用户数，$n$ 为标签总数。

## 多标签相关性分析

通过标签间的关联优化用户兴趣表示，分为 **内部相关** 和 **外部相关**。

### 内部相关性

同一用户内标签的共现频率越高，相关性越强（类似协同过滤）。计算方法如下：

1. **LIR（标签内部相关性）** 基于用户集合 $H$（同时拥有标签 $l_j$ 和 $l_k$
   的用户），计算加权 Jaccard 相似度： $$ \text{LIR}(l_j, l_k) = \frac{1}{|H|}
   \sum_{i \in H} \frac{w_{ij} w_{ik}}{w_{ij} + w_{ik} - w_{ij} w_{ik}} $$
   其中分母 $w_{ij} + w_{ik} - w_{ij} w_{ik}$
   近似“标签共现概率”，分子为共现权重的调和平均。

2. **归一化 N-LIR** 归一化确保每个标签与其他标签的相关性总和为
   1，便于比较。将相关性标准化到概率分布： $$ N-\text{LIR}(l_j, l_k) =
   \begin{cases} 1 & (j=k) \\ \frac{\text{LIR}(l_j, l_k)}{\sum_{j \neq k}
   \text{LIR}(l_j, l_k)} & (j \neq k) \end{cases} $$

### 外部相关性

通过其他用户的标签共现，发现跨用户的间接关联（类似图神经网络中的路径传播）。

计算方法如下：

1. **LOR（标签外部相关性）** 若存在中间标签 $l_q$，同时与 $l_j$ 和 $l_k$
   相关，则定义外部相关性： $$\text{LOR}(l_j, l_k | l_q) =
   \min\left(N-\text{LIR}(l_j, l_q), N-\text{LIR}(l_k, l_q)\right)$$
   它表明外部相关性强度由最弱的一侧决定（木桶效应）。
2. **归一化的 LOR** 对于两个标签 $l_j$ 和 $l_k$ 的外部相关性的归一化定义为
   $$N\text{-}\mathrm{LOR}(l_j, l_k) = \begin{cases} 0, & j = k \\
   \displaystyle\frac{\sum\limits_{l_q \in E} \mathrm{LOR}(l_j, l_k|l_q)}{|E|},
   & j \neq k \end{cases} \\[2ex] \text{其中 } E = \{ l_q \mid
   (N\text{-}\mathrm{LIR}(l_j, l_q) > 0) \ \land\
   (N\text{-}\mathrm{LIR}(l_k,l_q) > 0) \}$$

### 相关性定义

最终结合内部相关性和外部相关性，对于两个标签 $l_j$ 和 $l_k$ 的相关性可定义为

$$ \mathrm{LR}(l_j, l_k) = \begin{cases} 1, & j = k \\ \alpha \times
N\text{-}\mathrm{LOR}(l_j, l_k) + (1 - \alpha) \times N\text{-}\mathrm{LIR}(l_j,
l_k), & j ≠ k \end{cases} $$

其中的 $\alpha ∈[0,1]$ ，确定多标签间内相关性和外相关性的相对重要性。

## 用户 - 标签矩阵更新

这部分主要讲了**多用户社交关系建模**和**推荐算法的迭代过程**。

$M_{ut} = M_{ul} \times M_{lt}$

- $M_{ul}$：用户 - 标签矩阵，反映用户使用的标签。
- $M_{lt}$：标签 - 标签矩阵，表示标签和标签之间的相关关系。
- $M_{ut}$：用户 - 标签矩阵。

通过用户所用的词，间接构造用户与标签之间的关系，从而丰富用户的兴趣表示。因为标签是聚合的词，所以矩阵更稠密，表示效果更好。

## 多用户社交关系

### 多用户社交关系建模

核心思想是通过 **关注 - 被关注（Following/Followee）关系**
构建用户之间的相似度。社交关系被建模为向量 $Fg(u)$ 和 $Fr(u)$：

- $Fg(u)$：u 关注的人。
- $Fr(u)$：关注 u 的人。

相似度计算（使用余弦相似度）:

$$ sim(Fg(u_i), Fg(u_j)) = \frac{Fg(u_i) \cdot Fg(u_j)}{\|Fg(u_i)\| \cdot
\|Fg(u_j)\|} $$

$$ sim(Fr(u_i), Fr(u_j)) = \frac{Fr(u_i) \cdot Fr(u_j)}{\|Fr(u_i)\| \cdot
\|Fr(u_j)\|} $$

得到综合社交相似度

$$ sim(SR(u_i), SR(u_j)) = sim(Fg(u_i), Fg(u_j)) + sim(Fr(u_i), Fr(u_j)) $$

再进行归一化

$$ N\text{-}sim = \frac{sim - \min(sim)}{\max(sim) - \min(sim)} $$

然后计算社交关系矩阵 $M_{sr}$

- 构造一个 $N \times N$ 的社交相似度矩阵。
- 如果用户之间有社交关系，使用归一化相似度；如果没有直接关系，则为 0 或 1。

$$ m_{ij} = \begin{cases} N\text{-}sim(SR(u_i), SR(u_j)), & \text{若有社交关系}
\\ 1, & i = j \\ 0, & \text{否则} \end{cases} $$

## 迭代推荐算法

为了将社交关系与兴趣建模结合起来，提出一种基于 PageRank 风格的**迭代算法**。

$$ M_{ut}^{(t)} = \beta M_{ut} + (1 - \beta) M_{sr} \times M_{ut}^{(t-1)} $$

- $M_{ut}^{(t)}$：第 $t$ 轮迭代后的用户 - 标签矩阵。
- $M_{sr}$：社交关系矩阵。
- $\beta$：控制社交关系与原始标签关系的权重。

**类似 PageRank**，反复乘积进行迭代，直到收敛（矩阵变化小于阈值）。

## 微博推荐流程总结

算法总结如下步骤：

1. 构造用户词频。
2. 基于 clarity 和 TF 计算权重，得到用户标签。
3. 构造用户 - 标签矩阵。
4. 构造社交相似度矩阵。
5. 进行矩阵迭代更新（融合兴趣与社交关系）。
6. 得到推荐结果。

## 最终打分公式

$f(u_i, d_p) = \vec{E} \cdot (\vec{V_i})^T$

- $\vec{E}$：微博 $d_p$ 的标签向量。
- $\vec{V_i}$：用户 $u_i$ 的权重标签向量（由最终 $M_{ut}$ 得出）。

如果得分超过阈值 $\gamma_i$，推荐微博 $d_p$ 给用户 $u_i$。
